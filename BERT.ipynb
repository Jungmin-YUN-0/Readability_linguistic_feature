{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-bert\n",
      "  Using cached pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: tqdm in /home/jungmin/anaconda3/lib/python3.9/site-packages (from pytorch-pretrained-bert) (4.62.3)\n",
      "Requirement already satisfied: regex in /home/jungmin/anaconda3/lib/python3.9/site-packages (from pytorch-pretrained-bert) (2021.8.3)\n",
      "Requirement already satisfied: requests in /home/jungmin/anaconda3/lib/python3.9/site-packages (from pytorch-pretrained-bert) (2.26.0)\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.24.7-py3-none-any.whl (132 kB)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from pytorch-pretrained-bert) (1.11.0)\n",
      "Requirement already satisfied: numpy in /home/jungmin/anaconda3/lib/python3.9/site-packages (from pytorch-pretrained-bert) (1.22.4)\n",
      "Requirement already satisfied: typing-extensions in /home/jungmin/anaconda3/lib/python3.9/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.10.0.2)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Using cached s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
      "Collecting botocore<1.28.0,>=1.27.7\n",
      "  Using cached botocore-1.27.7-py3-none-any.whl (8.9 MB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from botocore<1.28.0,>=1.27.7->boto3->pytorch-pretrained-bert) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from botocore<1.28.0,>=1.27.7->boto3->pytorch-pretrained-bert) (1.26.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.7->boto3->pytorch-pretrained-bert) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
      "Successfully installed boto3-1.24.7 botocore-1.27.7 jmespath-1.0.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/jungmin/anaconda3/lib/python3.9/site-packages (4.19.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in /home/jungmin/anaconda3/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: filelock in /home/jungmin/anaconda3/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jungmin/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import gelu\n",
    "from transformers import (BertTokenizer, BertConfig,\n",
    "                          BertForSequenceClassification, BertPreTrainedModel,\n",
    "                          apply_chunking_to_forward, set_seed,\n",
    "                          )\n",
    "from transformers.modeling_outputs import (BaseModelOutputWithPastAndCrossAttentions,\n",
    "                                           BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "                                           SequenceClassifierOutput,\n",
    "                                           )\n",
    "                                           \n",
    "from pytorch_pretrained_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(r'./onestopeng/test.csv')\n",
    "#train = pd.read_csv(r'./onestopeng/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'./onestopeng/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = test.text.values.tolist()\n",
    "labels = test.label.values.tolist()\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = train.text.values.tolist()\n",
    "labels = train.label.values.tolist()\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in input_texts:\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        #pad_to_max_length = True,\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        truncation=True\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0).long()\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_tensors = torch.ones(510,512).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([510, 512]), torch.Size([510, 512]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, segments_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 510\n",
      "1 / 510\n",
      "2 / 510\n",
      "3 / 510\n",
      "4 / 510\n",
      "5 / 510\n",
      "6 / 510\n",
      "7 / 510\n",
      "8 / 510\n",
      "9 / 510\n",
      "10 / 510\n",
      "11 / 510\n",
      "12 / 510\n",
      "13 / 510\n",
      "14 / 510\n",
      "15 / 510\n",
      "16 / 510\n",
      "17 / 510\n",
      "18 / 510\n",
      "19 / 510\n",
      "20 / 510\n",
      "21 / 510\n",
      "22 / 510\n",
      "23 / 510\n",
      "24 / 510\n",
      "25 / 510\n",
      "26 / 510\n",
      "27 / 510\n",
      "28 / 510\n",
      "29 / 510\n",
      "30 / 510\n",
      "31 / 510\n",
      "32 / 510\n",
      "33 / 510\n",
      "34 / 510\n",
      "35 / 510\n",
      "36 / 510\n",
      "37 / 510\n",
      "38 / 510\n",
      "39 / 510\n",
      "40 / 510\n",
      "41 / 510\n",
      "42 / 510\n",
      "43 / 510\n",
      "44 / 510\n",
      "45 / 510\n",
      "46 / 510\n",
      "47 / 510\n",
      "48 / 510\n",
      "49 / 510\n",
      "50 / 510\n",
      "51 / 510\n",
      "52 / 510\n",
      "53 / 510\n",
      "54 / 510\n",
      "55 / 510\n",
      "56 / 510\n",
      "57 / 510\n",
      "58 / 510\n",
      "59 / 510\n",
      "60 / 510\n",
      "61 / 510\n",
      "62 / 510\n",
      "63 / 510\n",
      "64 / 510\n",
      "65 / 510\n",
      "66 / 510\n",
      "67 / 510\n",
      "68 / 510\n",
      "69 / 510\n",
      "70 / 510\n",
      "71 / 510\n",
      "72 / 510\n",
      "73 / 510\n",
      "74 / 510\n",
      "75 / 510\n",
      "76 / 510\n",
      "77 / 510\n",
      "78 / 510\n",
      "79 / 510\n",
      "80 / 510\n",
      "81 / 510\n",
      "82 / 510\n",
      "83 / 510\n",
      "84 / 510\n",
      "85 / 510\n",
      "86 / 510\n",
      "87 / 510\n",
      "88 / 510\n",
      "89 / 510\n",
      "90 / 510\n",
      "91 / 510\n",
      "92 / 510\n",
      "93 / 510\n",
      "94 / 510\n",
      "95 / 510\n",
      "96 / 510\n",
      "97 / 510\n",
      "98 / 510\n",
      "99 / 510\n",
      "100 / 510\n",
      "101 / 510\n",
      "102 / 510\n",
      "103 / 510\n",
      "104 / 510\n",
      "105 / 510\n",
      "106 / 510\n",
      "107 / 510\n",
      "108 / 510\n",
      "109 / 510\n",
      "110 / 510\n",
      "111 / 510\n",
      "112 / 510\n",
      "113 / 510\n",
      "114 / 510\n",
      "115 / 510\n",
      "116 / 510\n",
      "117 / 510\n",
      "118 / 510\n",
      "119 / 510\n",
      "120 / 510\n",
      "121 / 510\n",
      "122 / 510\n",
      "123 / 510\n",
      "124 / 510\n",
      "125 / 510\n",
      "126 / 510\n",
      "127 / 510\n",
      "128 / 510\n",
      "129 / 510\n",
      "130 / 510\n",
      "131 / 510\n",
      "132 / 510\n",
      "133 / 510\n",
      "134 / 510\n",
      "135 / 510\n",
      "136 / 510\n",
      "137 / 510\n",
      "138 / 510\n",
      "139 / 510\n",
      "140 / 510\n",
      "141 / 510\n",
      "142 / 510\n",
      "143 / 510\n",
      "144 / 510\n",
      "145 / 510\n",
      "146 / 510\n",
      "147 / 510\n",
      "148 / 510\n",
      "149 / 510\n",
      "150 / 510\n",
      "151 / 510\n",
      "152 / 510\n",
      "153 / 510\n",
      "154 / 510\n",
      "155 / 510\n",
      "156 / 510\n",
      "157 / 510\n",
      "158 / 510\n",
      "159 / 510\n",
      "160 / 510\n",
      "161 / 510\n",
      "162 / 510\n",
      "163 / 510\n",
      "164 / 510\n",
      "165 / 510\n",
      "166 / 510\n",
      "167 / 510\n",
      "168 / 510\n",
      "169 / 510\n",
      "170 / 510\n",
      "171 / 510\n",
      "172 / 510\n",
      "173 / 510\n",
      "174 / 510\n",
      "175 / 510\n",
      "176 / 510\n",
      "177 / 510\n",
      "178 / 510\n",
      "179 / 510\n",
      "180 / 510\n",
      "181 / 510\n",
      "182 / 510\n",
      "183 / 510\n",
      "184 / 510\n",
      "185 / 510\n",
      "186 / 510\n",
      "187 / 510\n",
      "188 / 510\n",
      "189 / 510\n",
      "190 / 510\n",
      "191 / 510\n",
      "192 / 510\n",
      "193 / 510\n",
      "194 / 510\n",
      "195 / 510\n",
      "196 / 510\n",
      "197 / 510\n",
      "198 / 510\n",
      "199 / 510\n",
      "200 / 510\n",
      "201 / 510\n",
      "202 / 510\n",
      "203 / 510\n",
      "204 / 510\n",
      "205 / 510\n",
      "206 / 510\n",
      "207 / 510\n",
      "208 / 510\n",
      "209 / 510\n",
      "210 / 510\n",
      "211 / 510\n",
      "212 / 510\n",
      "213 / 510\n",
      "214 / 510\n",
      "215 / 510\n",
      "216 / 510\n",
      "217 / 510\n",
      "218 / 510\n",
      "219 / 510\n",
      "220 / 510\n",
      "221 / 510\n",
      "222 / 510\n",
      "223 / 510\n",
      "224 / 510\n",
      "225 / 510\n",
      "226 / 510\n",
      "227 / 510\n",
      "228 / 510\n",
      "229 / 510\n",
      "230 / 510\n",
      "231 / 510\n",
      "232 / 510\n",
      "233 / 510\n",
      "234 / 510\n",
      "235 / 510\n",
      "236 / 510\n",
      "237 / 510\n",
      "238 / 510\n",
      "239 / 510\n",
      "240 / 510\n",
      "241 / 510\n",
      "242 / 510\n",
      "243 / 510\n",
      "244 / 510\n",
      "245 / 510\n",
      "246 / 510\n",
      "247 / 510\n",
      "248 / 510\n",
      "249 / 510\n",
      "250 / 510\n",
      "251 / 510\n",
      "252 / 510\n",
      "253 / 510\n",
      "254 / 510\n",
      "255 / 510\n",
      "256 / 510\n",
      "257 / 510\n",
      "258 / 510\n",
      "259 / 510\n",
      "260 / 510\n",
      "261 / 510\n",
      "262 / 510\n",
      "263 / 510\n",
      "264 / 510\n",
      "265 / 510\n",
      "266 / 510\n",
      "267 / 510\n",
      "268 / 510\n",
      "269 / 510\n",
      "270 / 510\n",
      "271 / 510\n",
      "272 / 510\n",
      "273 / 510\n",
      "274 / 510\n",
      "275 / 510\n",
      "276 / 510\n",
      "277 / 510\n",
      "278 / 510\n",
      "279 / 510\n",
      "280 / 510\n",
      "281 / 510\n",
      "282 / 510\n",
      "283 / 510\n",
      "284 / 510\n",
      "285 / 510\n",
      "286 / 510\n",
      "287 / 510\n",
      "288 / 510\n",
      "289 / 510\n",
      "290 / 510\n",
      "291 / 510\n",
      "292 / 510\n",
      "293 / 510\n",
      "294 / 510\n",
      "295 / 510\n",
      "296 / 510\n",
      "297 / 510\n",
      "298 / 510\n",
      "299 / 510\n",
      "300 / 510\n",
      "301 / 510\n",
      "302 / 510\n",
      "303 / 510\n",
      "304 / 510\n",
      "305 / 510\n",
      "306 / 510\n",
      "307 / 510\n",
      "308 / 510\n",
      "309 / 510\n",
      "310 / 510\n",
      "311 / 510\n",
      "312 / 510\n",
      "313 / 510\n",
      "314 / 510\n",
      "315 / 510\n",
      "316 / 510\n",
      "317 / 510\n",
      "318 / 510\n",
      "319 / 510\n",
      "320 / 510\n",
      "321 / 510\n",
      "322 / 510\n",
      "323 / 510\n",
      "324 / 510\n",
      "325 / 510\n",
      "326 / 510\n",
      "327 / 510\n",
      "328 / 510\n",
      "329 / 510\n",
      "330 / 510\n",
      "331 / 510\n",
      "332 / 510\n",
      "333 / 510\n",
      "334 / 510\n",
      "335 / 510\n",
      "336 / 510\n",
      "337 / 510\n",
      "338 / 510\n",
      "339 / 510\n",
      "340 / 510\n",
      "341 / 510\n",
      "342 / 510\n",
      "343 / 510\n",
      "344 / 510\n",
      "345 / 510\n",
      "346 / 510\n",
      "347 / 510\n",
      "348 / 510\n",
      "349 / 510\n",
      "350 / 510\n",
      "351 / 510\n",
      "352 / 510\n",
      "353 / 510\n",
      "354 / 510\n",
      "355 / 510\n",
      "356 / 510\n",
      "357 / 510\n",
      "358 / 510\n",
      "359 / 510\n",
      "360 / 510\n",
      "361 / 510\n",
      "362 / 510\n",
      "363 / 510\n",
      "364 / 510\n",
      "365 / 510\n",
      "366 / 510\n",
      "367 / 510\n",
      "368 / 510\n",
      "369 / 510\n",
      "370 / 510\n",
      "371 / 510\n",
      "372 / 510\n",
      "373 / 510\n",
      "374 / 510\n",
      "375 / 510\n",
      "376 / 510\n",
      "377 / 510\n",
      "378 / 510\n",
      "379 / 510\n",
      "380 / 510\n",
      "381 / 510\n",
      "382 / 510\n",
      "383 / 510\n",
      "384 / 510\n",
      "385 / 510\n",
      "386 / 510\n",
      "387 / 510\n",
      "388 / 510\n",
      "389 / 510\n",
      "390 / 510\n",
      "391 / 510\n",
      "392 / 510\n",
      "393 / 510\n",
      "394 / 510\n",
      "395 / 510\n",
      "396 / 510\n",
      "397 / 510\n",
      "398 / 510\n",
      "399 / 510\n",
      "400 / 510\n",
      "401 / 510\n",
      "402 / 510\n",
      "403 / 510\n",
      "404 / 510\n",
      "405 / 510\n",
      "406 / 510\n",
      "407 / 510\n",
      "408 / 510\n",
      "409 / 510\n",
      "410 / 510\n",
      "411 / 510\n",
      "412 / 510\n",
      "413 / 510\n",
      "414 / 510\n",
      "415 / 510\n",
      "416 / 510\n",
      "417 / 510\n",
      "418 / 510\n",
      "419 / 510\n",
      "420 / 510\n",
      "421 / 510\n",
      "422 / 510\n",
      "423 / 510\n",
      "424 / 510\n",
      "425 / 510\n",
      "426 / 510\n",
      "427 / 510\n",
      "428 / 510\n",
      "429 / 510\n",
      "430 / 510\n",
      "431 / 510\n",
      "432 / 510\n",
      "433 / 510\n",
      "434 / 510\n",
      "435 / 510\n",
      "436 / 510\n",
      "437 / 510\n",
      "438 / 510\n",
      "439 / 510\n",
      "440 / 510\n",
      "441 / 510\n",
      "442 / 510\n",
      "443 / 510\n",
      "444 / 510\n",
      "445 / 510\n",
      "446 / 510\n",
      "447 / 510\n",
      "448 / 510\n",
      "449 / 510\n",
      "450 / 510\n",
      "451 / 510\n",
      "452 / 510\n",
      "453 / 510\n",
      "454 / 510\n",
      "455 / 510\n",
      "456 / 510\n",
      "457 / 510\n",
      "458 / 510\n",
      "459 / 510\n",
      "460 / 510\n",
      "461 / 510\n",
      "462 / 510\n",
      "463 / 510\n",
      "464 / 510\n",
      "465 / 510\n",
      "466 / 510\n",
      "467 / 510\n",
      "468 / 510\n",
      "469 / 510\n",
      "470 / 510\n",
      "471 / 510\n",
      "472 / 510\n",
      "473 / 510\n",
      "474 / 510\n",
      "475 / 510\n",
      "476 / 510\n",
      "477 / 510\n",
      "478 / 510\n",
      "479 / 510\n",
      "480 / 510\n",
      "481 / 510\n",
      "482 / 510\n",
      "483 / 510\n",
      "484 / 510\n",
      "485 / 510\n",
      "486 / 510\n",
      "487 / 510\n",
      "488 / 510\n",
      "489 / 510\n",
      "490 / 510\n",
      "491 / 510\n",
      "492 / 510\n",
      "493 / 510\n",
      "494 / 510\n",
      "495 / 510\n",
      "496 / 510\n",
      "497 / 510\n",
      "498 / 510\n",
      "499 / 510\n",
      "500 / 510\n",
      "501 / 510\n",
      "502 / 510\n",
      "503 / 510\n",
      "504 / 510\n",
      "505 / 510\n",
      "506 / 510\n",
      "507 / 510\n",
      "508 / 510\n",
      "509 / 510\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0,510):\n",
    "  with torch.no_grad():\n",
    "    print(i,\"/\",510)\n",
    "    encoded_layers, _ = model(input_ids[i,:].unsqueeze(0), segments_tensors[i,:].unsqueeze(0))\n",
    "    embeddings.append(encoded_layers[11][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.stack(embeddings, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(567, 512, 768)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(emb, open(\"Embedding_OSE.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"Embedding_OSE.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(567, 512, 768)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "466d30325bbd31ae2f76643e3a6ce78283c92d4bc70a4ead01364c67bcad87c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
